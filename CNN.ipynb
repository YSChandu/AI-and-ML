{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ba9016-f89e-4bae-a1d9-43825ff9ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f751bfb-a04a-444c-8538-bab5ace449a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07e76e1-621a-459a-a173-8eb7787fae81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfeaturewise_center\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msamplewise_center\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfeaturewise_std_normalization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msamplewise_std_normalization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mzca_whitening\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mzca_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrotation_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwidth_shift_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mheight_shift_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbrightness_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshear_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mzoom_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchannel_shift_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfill_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mhorizontal_flip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvertical_flip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrescale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpreprocessing_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minterpolation_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Generate batches of tensor image data with real-time data augmentation.\n",
       "\n",
       "Deprecated: `tf.keras.preprocessing.image.ImageDataGenerator` is not\n",
       "recommended for new code. Prefer loading images with\n",
       "`tf.keras.utils.image_dataset_from_directory` and transforming the output\n",
       "`tf.data.Dataset` with preprocessing layers. For more information, see the\n",
       "tutorials for [loading images](\n",
       "https://www.tensorflow.org/tutorials/load_data/images) and\n",
       "[augmenting images](\n",
       "https://www.tensorflow.org/tutorials/images/data_augmentation), as well as\n",
       "the [preprocessing layer guide](\n",
       "https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
       "\n",
       " The data will be looped over (in batches).\n",
       "\n",
       "Args:\n",
       "    featurewise_center: Boolean. Set input mean to 0 over the dataset,\n",
       "      feature-wise.\n",
       "    samplewise_center: Boolean. Set each sample mean to 0.\n",
       "    featurewise_std_normalization: Boolean. Divide inputs by std of the\n",
       "      dataset, feature-wise.\n",
       "    samplewise_std_normalization: Boolean. Divide each input by its std.\n",
       "    zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
       "    zca_whitening: Boolean. Apply ZCA whitening.\n",
       "    rotation_range: Int. Degree range for random rotations.\n",
       "    width_shift_range: Float, 1-D array-like or int\n",
       "        - float: fraction of total width, if < 1, or pixels if >= 1.\n",
       "        - 1-D array-like: random elements from the array.\n",
       "        - int: integer number of pixels from interval `(-width_shift_range,\n",
       "          +width_shift_range)` - With `width_shift_range=2` possible values\n",
       "          are integers `[-1, 0, +1]`, same as with `width_shift_range=[-1,\n",
       "          0, +1]`, while with `width_shift_range=1.0` possible values are\n",
       "          floats in the interval [-1.0, +1.0).\n",
       "    height_shift_range: Float, 1-D array-like or int\n",
       "        - float: fraction of total height, if < 1, or pixels if >= 1.\n",
       "        - 1-D array-like: random elements from the array.\n",
       "        - int: integer number of pixels from interval `(-height_shift_range,\n",
       "          +height_shift_range)` - With `height_shift_range=2` possible\n",
       "          values are integers `[-1, 0, +1]`, same as with\n",
       "          `height_shift_range=[-1, 0, +1]`, while with\n",
       "          `height_shift_range=1.0` possible values are floats in the\n",
       "          interval [-1.0, +1.0).\n",
       "    brightness_range: Tuple or list of two floats. Range for picking a\n",
       "      brightness shift value from.\n",
       "    shear_range: Float. Shear Intensity (Shear angle in counter-clockwise\n",
       "      direction in degrees)\n",
       "    zoom_range: Float or [lower, upper]. Range for random zoom. If a float,\n",
       "      `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
       "    channel_shift_range: Float. Range for random channel shifts.\n",
       "    fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default\n",
       "      is 'nearest'. Points outside the boundaries of the input are filled\n",
       "      according to the given mode:\n",
       "        - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
       "        - 'nearest':  aaaaaaaa|abcd|dddddddd\n",
       "        - 'reflect':  abcddcba|abcd|dcbaabcd\n",
       "        - 'wrap':  abcdabcd|abcd|abcdabcd\n",
       "    cval: Float or Int. Value used for points outside the boundaries when\n",
       "      `fill_mode = \"constant\"`.\n",
       "    horizontal_flip: Boolean. Randomly flip inputs horizontally.\n",
       "    vertical_flip: Boolean. Randomly flip inputs vertically.\n",
       "    rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
       "      is applied, otherwise we multiply the data by the value provided\n",
       "      (after applying all other transformations).\n",
       "    preprocessing_function: function that will be applied on each input. The\n",
       "      function will run after the image is resized and augmented.\n",
       "        The function should take one argument: one image (Numpy tensor with\n",
       "          rank 3), and should output a Numpy tensor with the same shape.\n",
       "    data_format: Image data format, either \"channels_first\" or\n",
       "      \"channels_last\". \"channels_last\" mode means that the images should\n",
       "      have shape `(samples, height, width, channels)`, \"channels_first\" mode\n",
       "      means that the images should have shape `(samples, channels, height,\n",
       "      width)`.  It defaults to the `image_data_format` value found in your\n",
       "      Keras config file at `~/.keras/keras.json`. If you never set it, then\n",
       "      it will be \"channels_last\".\n",
       "    validation_split: Float. Fraction of images reserved for validation\n",
       "      (strictly between 0 and 1).\n",
       "    dtype: Dtype to use for the generated arrays.\n",
       "\n",
       "Raises:\n",
       "  ValueError: If the value of the argument, `data_format` is other than\n",
       "        `\"channels_last\"` or `\"channels_first\"`.\n",
       "  ValueError: If the value of the argument, `validation_split` > 1\n",
       "        or `validation_split` < 0.\n",
       "\n",
       "Examples:\n",
       "\n",
       "Example of using `.flow(x, y)`:\n",
       "\n",
       "```python\n",
       "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
       "y_train = utils.to_categorical(y_train, num_classes)\n",
       "y_test = utils.to_categorical(y_test, num_classes)\n",
       "datagen = ImageDataGenerator(\n",
       "    featurewise_center=True,\n",
       "    featurewise_std_normalization=True,\n",
       "    rotation_range=20,\n",
       "    width_shift_range=0.2,\n",
       "    height_shift_range=0.2,\n",
       "    horizontal_flip=True,\n",
       "    validation_split=0.2)\n",
       "# compute quantities required for featurewise normalization\n",
       "# (std, mean, and principal components if ZCA whitening is applied)\n",
       "datagen.fit(x_train)\n",
       "# fits the model on batches with real-time data augmentation:\n",
       "model.fit(datagen.flow(x_train, y_train, batch_size=32,\n",
       "         subset='training'),\n",
       "         validation_data=datagen.flow(x_train, y_train,\n",
       "         batch_size=8, subset='validation'),\n",
       "         steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
       "# here's a more \"manual\" example\n",
       "for e in range(epochs):\n",
       "    print('Epoch', e)\n",
       "    batches = 0\n",
       "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
       "        model.fit(x_batch, y_batch)\n",
       "        batches += 1\n",
       "        if batches >= len(x_train) / 32:\n",
       "            # we need to break the loop by hand because\n",
       "            # the generator loops indefinitely\n",
       "            break\n",
       "```\n",
       "\n",
       "Example of using `.flow_from_directory(directory)`:\n",
       "\n",
       "```python\n",
       "train_datagen = ImageDataGenerator(\n",
       "        rescale=1./255,\n",
       "        shear_range=0.2,\n",
       "        zoom_range=0.2,\n",
       "        horizontal_flip=True)\n",
       "test_datagen = ImageDataGenerator(rescale=1./255)\n",
       "train_generator = train_datagen.flow_from_directory(\n",
       "        'data/train',\n",
       "        target_size=(150, 150),\n",
       "        batch_size=32,\n",
       "        class_mode='binary')\n",
       "validation_generator = test_datagen.flow_from_directory(\n",
       "        'data/validation',\n",
       "        target_size=(150, 150),\n",
       "        batch_size=32,\n",
       "        class_mode='binary')\n",
       "model.fit(\n",
       "        train_generator,\n",
       "        steps_per_epoch=2000,\n",
       "        epochs=50,\n",
       "        validation_data=validation_generator,\n",
       "        validation_steps=800)\n",
       "```\n",
       "\n",
       "Example of transforming images and masks together.\n",
       "\n",
       "```python\n",
       "# we create two instances with the same arguments\n",
       "data_gen_args = dict(featurewise_center=True,\n",
       "                     featurewise_std_normalization=True,\n",
       "                     rotation_range=90,\n",
       "                     width_shift_range=0.1,\n",
       "                     height_shift_range=0.1,\n",
       "                     zoom_range=0.2)\n",
       "image_datagen = ImageDataGenerator(**data_gen_args)\n",
       "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
       "# Provide the same seed and keyword arguments to the fit and flow methods\n",
       "seed = 1\n",
       "image_datagen.fit(images, augment=True, seed=seed)\n",
       "mask_datagen.fit(masks, augment=True, seed=seed)\n",
       "image_generator = image_datagen.flow_from_directory(\n",
       "    'data/images',\n",
       "    class_mode=None,\n",
       "    seed=seed)\n",
       "mask_generator = mask_datagen.flow_from_directory(\n",
       "    'data/masks',\n",
       "    class_mode=None,\n",
       "    seed=seed)\n",
       "# combine generators into one which yields image and masks\n",
       "train_generator = zip(image_generator, mask_generator)\n",
       "model.fit(\n",
       "    train_generator,\n",
       "    steps_per_epoch=2000,\n",
       "    epochs=50)\n",
       "```\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\dell\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\preprocessing\\image.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc7a04c-285e-4be2-8ce7-d9c51d662bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255 , shear_range=0.2 ,zoom_range=0.2,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd1554ac-675b-4697-9433-454eba2080d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(r\"D:\\Courses\\Ml_AtoZ\\Machine Learning-A-Z-Codes-Datasets\\Machine Learning A-Z (Codes and Datasets)\\Part 8 - Deep Learning\\Section 40 - Convolutional Neural Networks (CNN)\\dataset\\dataset\\training_set\",\n",
    "                                                 target_size=(64,64),\n",
    "                                                 batch_size=10,\n",
    "                                                 class_mode='binary'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2fb632e-cd27-4bd1-bc84-9bc0d1665961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set=test_datagen.flow_from_directory(r\"D:\\Courses\\Ml_AtoZ\\Machine Learning-A-Z-Codes-Datasets\\Machine Learning A-Z (Codes and Datasets)\\Part 8 - Deep Learning\\Section 40 - Convolutional Neural Networks (CNN)\\dataset\\dataset\\test_set\",\n",
    "                                                 target_size=(64,64),\n",
    "                                                 batch_size=10,\n",
    "                                                 class_mode='binary'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283640da-3dce-4088-ba4c-448c080de561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the  CNN\n",
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55f43c0-54bc-4079-a257-c59d58191910",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,activation='relu',input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58657153-6e7b-41ee-8ec7-5ab1640b4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3b54e9-9514-4413-a1bc-ac4631476f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13bf17b4-99a4-45a1-b6cd-4aaa3751c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening \n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab688e7c-599b-47eb-8821-9248fe6b90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=28,activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ac9b6a6-f766-4d34-9335-e21a6c74a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d82fe7-1d94-4f89-95a0-3f352a24c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(x=training_set, validation_data =test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f632d3-5f54-4267-b0ac-74568a09ec24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
